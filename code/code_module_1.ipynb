{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "# import textract\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-z0-9-]*\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_text(file_name):\n",
    "    pdfFileObj = open(file_name, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    num_pages = pdfReader.numPages\n",
    "    full_text = \"\"\n",
    "    for i in range(num_pages):\n",
    "        pageObj = pdfReader.getPage(i)\n",
    "        try:            \n",
    "            full_text += pageObj.extractText()\n",
    "        except:\n",
    "            pass\n",
    "    pdfFileObj.close()\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "doc_text = get_file_text('../Patent_data_set/US3323996.pdf') #get raw document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cancer_list(cancer_list_file):\n",
    "    cancers = open(cancer_list_file, 'r').read().lower().split(\", \")\n",
    "    size_based_canc_list = [list() for j in range(4)]\n",
    "    for cancer in cancers:\n",
    "        c = lemmatizer.lemmatize(cancer)\n",
    "        size_based_canc_list[len(c.split(\" \")) - 1].append(c)\n",
    "    return size_based_canc_list\n",
    "\n",
    "canc_list = get_cancer_list(\"cancer list.txt\") ## get names of all types of cancers (total 176 types), \n",
    "# canc_list has separate components list for unigrams, bigrams, trigrams and 4grams cancer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for i in canc_list:\n",
    "    total += len(i)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_tokens(data):\n",
    "    tokens = tokenizer.tokenize(data)\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        elif token == '.' or token == '_':\n",
    "            continue\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        normalized_tokens.append(token)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tokens = get_normalized_tokens(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3688"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalized_tokens) # retrieving tokens after normalization from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_tokens(tokens_list, n):\n",
    "    grams = []\n",
    "    for ngram in ngrams(tokens_list, n):\n",
    "        grams.append(' '.join(i for i in ngram))\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This grams list contains 4 lists - 1st of unigrams, 2nd of bigrams, 3rd of trigrams, 4th of 4grams and 5th of 5grams \n",
    "# of the retrieveed document\n",
    "grams = [get_ngrams_tokens(normalized_tokens, 1), get_ngrams_tokens(normalized_tokens, 2), \n",
    "         get_ngrams_tokens(normalized_tokens, 3), get_ngrams_tokens(normalized_tokens, 4)]\n",
    "# for i in grams:\n",
    "#     print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cancers_from_doc(doc_grams, cancers_list):\n",
    "    found_4gr = dict()\n",
    "    for canc in cancers_list[3]:\n",
    "        t = doc_grams[3].count(canc)\n",
    "        if(t):\n",
    "            found_4gr[canc] = t\n",
    "    found_3gr = dict()\n",
    "    for canc in cancers_list[2]:\n",
    "        flag = False\n",
    "        for k in found_4gr.keys():\n",
    "            if canc in k:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            continue        \n",
    "        t = doc_grams[2].count(canc)\n",
    "        if(t):\n",
    "            found_3gr[canc] = t\n",
    "    found_2gr = dict()\n",
    "    for canc in cancers_list[1]:\n",
    "        flag = False\n",
    "        for k in found_4gr.keys():\n",
    "            if canc in k:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            continue        \n",
    "        for k in found_3gr.keys():\n",
    "            if canc in k:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            continue        \n",
    "        t = doc_grams[1].count(canc)\n",
    "        if(t):\n",
    "            found_2gr[canc] = t\n",
    "    found_1gr = dict()\n",
    "    for canc in cancers_list[0]:\n",
    "        flag = False\n",
    "        for k in found_4gr.keys():\n",
    "            if canc in k:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            continue        \n",
    "        for k in found_3gr.keys():\n",
    "            if canc in k:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            continue        \n",
    "        for k in found_2gr.keys():\n",
    "            if canc in k:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            continue        \n",
    "        t = doc_grams[0].count(canc)\n",
    "        if(t):\n",
    "            found_1gr[canc] = t\n",
    "    return found_4gr, found_3gr, found_2gr, found_1gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_4gr, found_3gr, found_2gr, found_1gr = find_cancers_from_doc(grams, canc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------+\n",
      "| S.no. | Cancer Type | Frequency |\n",
      "+-------+-------------+-----------+\n",
      "|   1   |   sarcoma   |     4     |\n",
      "|   2   |  carcinoma  |     4     |\n",
      "+-------+-------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "x = PrettyTable()\n",
    "index = 1\n",
    "cancers = list(found_1gr.items()) + list(found_2gr.items()) + list(found_3gr.items()) + list(found_4gr.items())\n",
    "cancers.sort(key = lambda x: x[1], reverse = True)\n",
    "x.field_names = [\"S.no.\", \"Cancer Type\"] + [\"Frequency\"]\n",
    "for cancer in cancers:\n",
    "    x.add_row([index, cancer[0], cancer[1]])\n",
    "    index += 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_virus_names(doc_grams):\n",
    "    grams_v = [list() for i in range(3)]\n",
    "    for unigram in doc_grams[0]:\n",
    "        if 'virus' in unigram and unigram != 'virus':\n",
    "            grams_v[0].append(unigram)\n",
    "    for i in range(1,3):\n",
    "        for gram in doc_grams[i]:\n",
    "            tok_gram = gram.split(\" \")\n",
    "#             if tok_gram[-1] == 'virus':\n",
    "            if 'virus' in tok_gram[-1]:\n",
    "                grams_v[i].append(gram)\n",
    "        i += 1\n",
    "    return grams_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams_v = get_virus_names(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+\n",
      "| S.no. |      Virus      | Frequency |\n",
      "+-------+-----------------+-----------+\n",
      "|   1   | influenza virus |     12    |\n",
      "+-------+-----------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "x = PrettyTable()\n",
    "viruses = grams_v[0] + grams_v[1] + grams_v[2]\n",
    "index = 1\n",
    "x.field_names = [\"S.no.\", \"Virus\"] + [\"Frequency\"]\n",
    "for k,v in Counter(viruses).most_common():\n",
    "    if v>2:\n",
    "        x.add_row([index, k, v])\n",
    "        index += 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
