{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2,os,sys,joblib\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-z0-9-]*\\w+')\n",
    "# import textract, re\n",
    "# import pdfminer\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text): # to lower case parameter string\n",
    "    return text.lower()\n",
    "\n",
    "def get_file_paragraphs(file_name, fig_no):\n",
    "    \"\"\"\n",
    "    This function reads the file specified in parameter and using the figure no. ,\n",
    "    it identifies that section of the document text in which the experiment is \n",
    "    diccussed whose results are specified in the figure\n",
    "    \"\"\"\n",
    "    pdfFileObj = open(file_name, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    num_pages = pdfReader.numPages\n",
    "    full_text = \"\"\n",
    "    fig = lowercase(fig_no.split(' ')[0])\n",
    "    fig_value = fig_no.split(' ')[1]\n",
    "#     print(num_pages)\n",
    "    for i in range(4, num_pages):\n",
    "#         print('page:',i)\n",
    "        pageObj = pdfReader.getPage(i)\n",
    "        try:\n",
    "            page_text = lowercase(pageObj.extractText())\n",
    "            sentences = sent_tokenize(page_text)\n",
    "#             if(i==20):\n",
    "#                 print(sentences)\n",
    "            para = ''\n",
    "            before =5\n",
    "            after = 8\n",
    "            for i in range(len(sentences)):\n",
    "                j=i-1\n",
    "                if((sentences[i][-4:]=='fig.' or sentences[i][-5:]=='fig .' or sentences[i][-5:]=='figs.' or sentences[i][-6:]=='figs .') and (fig_value in sentences[i+1][:4])):\n",
    "#                     print('yes')\n",
    "                    if(i-before < 0):\n",
    "                        stop = -1\n",
    "                    else:\n",
    "                        stop = i-before\n",
    "                    for j in range(i-1, stop, -1):\n",
    "                        para = sentences[j] + para\n",
    "\n",
    "                    para += sentences[i]\n",
    "\n",
    "                    if(i+after >= len(sentences)):\n",
    "                        stop = len(sentences)\n",
    "                    else:\n",
    "                        stop = i+after+1\n",
    "                    for j in range(i+1, stop, +1):\n",
    "                        para = para + sentences[j]\n",
    "            full_text+=para\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    pdfFileObj.close()\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_xl = pd.read_excel(r'annotated_graph_data.xlsx')\n",
    "records = code_xl.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a dictionary to store feature values retrieved from the filtered patents\n",
    "\"\"\"\n",
    "data = {}\n",
    "for row in records:\n",
    "    patent_id = row[1]\n",
    "    fig_id = row[2]\n",
    "    tumor_cell = row[3]\n",
    "    chemo_agent_virus = row[4]\n",
    "    y_value = row[5]\n",
    "    x_value = row[6]\n",
    "    \n",
    "    if(patent_id not in data):\n",
    "        data[patent_id]={}\n",
    "    if(fig_id not in data[patent_id]):\n",
    "        data[patent_id][fig_id]={'chemo/virus':[chemo_agent_virus],\n",
    "                                'tumor/cell_survival':[tumor_cell],# 0 or 1\n",
    "                                'days':[x_value],\n",
    "                                'tumor_size_value/cell_survival_value':[y_value],\n",
    "                                'para':\"\",\n",
    "                                'experiment_type':\"\",\n",
    "                                'species':\"\",\n",
    "                                \"cancer_type\":\"\",\n",
    "                                \"cell_lines\":[],\n",
    "                                \"quantity\":\"\"}      \n",
    "    else:\n",
    "        data[patent_id][fig_id]['chemo/virus'].append(chemo_agent_virus)\n",
    "        data[patent_id][fig_id]['tumor/cell_survival'].append(tumor_cell)\n",
    "        data[patent_id][fig_id]['days'].append(x_value)\n",
    "        data[patent_id][fig_id]['tumor_size_value/cell_survival_value'].append(y_value)\n",
    "#         data[patent_id][fig_id]['para'].append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read patent with specidied patent ids \n",
    "\"\"\"\n",
    "for patent_id in data:\n",
    "    for key in data[patent_id]: # key is fig_id\n",
    "#         print(patent_id, key)\n",
    "        path = '../Patent_data_set'\n",
    "        file_path = os.path.join(path,patent_id)\n",
    "        information = get_file_paragraphs(file_path,key)\n",
    "        data[patent_id][key]['para']=information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_type(paragraph, tumor_cell_survival_list):\n",
    "    \"\"\"\n",
    "    function to determine type off experiment performed - invivo or invitro\n",
    "    from text paragraph\n",
    "    \"\"\"\n",
    "    if(('invivo' in paragraph) or ('in vivo' in paragraph) or ('inject' in paragraph)):\n",
    "        return \"in vivo\"\n",
    "    elif(('invitro' in paragraph) or ('in vitro' in paragraph)):\n",
    "        return \"in vitro\"\n",
    "    else:\n",
    "        exp_type = sum(tumor_cell_survival_list)\n",
    "        if(exp_type==0):\n",
    "            return \"in vivo\"\n",
    "        else:\n",
    "            return \"in vitro\"\n",
    "        \n",
    "def get_species(paragraph,exp_type,invivolist): # only for invivo experiments\n",
    "    if(exp_type=='in vivo'):\n",
    "        for animal in invivolist:\n",
    "            if(animal in paragraph):\n",
    "                return animal\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cancer_list(cancer_list_file):\n",
    "    \"\"\"\n",
    "    read all cancer types from the 'cancer list' file\n",
    "    \"\"\"\n",
    "    cancers = open(cancer_list_file, 'r').read().lower().split(\", \")\n",
    "    size_based_canc_list = [list() for j in range(4)]\n",
    "    for cancer in cancers:\n",
    "        c = lemmatizer.lemmatize(cancer)\n",
    "        size_based_canc_list[len(c.split(\" \")) - 1].append(c)\n",
    "    return size_based_canc_list\n",
    "\n",
    "\n",
    "def get_cancer_type(paragraph, caner_list):\n",
    "    \"\"\"\n",
    "    put different cancer types in different lists based on their no. pf tokens\n",
    "    \"\"\"\n",
    "    possible_cancers=[]\n",
    "    for n_gram_list in caner_list:\n",
    "        for gram in n_gram_list:\n",
    "            possible_cancers.append((paragraph.count(gram),gram))\n",
    "    possible_cancers.sort()\n",
    "    return possible_cancers[-1][1]\n",
    "\n",
    "def get_cell_lines(paragraph,all_cell_lines):\n",
    "    \"\"\"\n",
    "    read all cell lines from saved list 'all_cell_lines.sav'\n",
    "    \"\"\"\n",
    "    temp_1=[]\n",
    "    temp_2=[]\n",
    "    for cell_line in all_cell_lines:\n",
    "        if(cell_line[0] in paragraph):\n",
    "            if(cell_line[0] not in temp_1 and len(cell_line[0])>2):\n",
    "                temp_1.append(cell_line[0])\n",
    "                temp_2.append(cell_line)\n",
    "    return temp_2\n",
    "\n",
    "def get_ngrams_tokens(tokens_list, n):\n",
    "    \"\"\"\n",
    "    function to break string to tokens \n",
    "    \"\"\"\n",
    "    grams = []\n",
    "    for ngram in ngrams(tokens_list, n):\n",
    "        grams.append(' '.join(i for i in ngram))\n",
    "    return grams\n",
    "\n",
    "def num_there(s):\n",
    "    \"\"\"\n",
    "    function to check if a given token has numeric character\n",
    "    \"\"\"\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "def get_quantity(paragraph,quantity_list_unigram):\n",
    "    \"\"\"\n",
    "    Function to get quantity of chemotherapic agent/ oncolyic virus strain used\n",
    "    from text paragraph\n",
    "    \"\"\"\n",
    "    result=\"\"\n",
    "    tokens = tokenizer.tokenize(paragraph)    \n",
    "    unigram = get_ngrams_tokens(tokens,1)\n",
    "    for quantity in quantity_list_unigram:\n",
    "        for i in range(len(unigram)):\n",
    "            if(quantity in unigram[i]):\n",
    "                x = unigram[i-1]\n",
    "                if(num_there(x)):\n",
    "                    if(quantity=='multiplicity' and unigram[i+1]=='of'):\n",
    "                        result = unigram[i-1] + ' '+unigram[i] + ' '+ unigram[i+1]+ ' '+unigram[i+2]\n",
    "                    else:\n",
    "                        result = unigram[i-1]+ ' '+unigram[i]\n",
    "                    \n",
    "                elif(num_there(unigram[i])):\n",
    "                    result = unigram[i]\n",
    "            if(unigram[i]=='moi' and len(result)==0):\n",
    "                if(unigram[i+1]=='of' and num_there(unigram[i+2])):\n",
    "                    result = unigram[i] + ' '+ unigram[i+1]+ ' '+unigram[i+2]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "invivo_animals = ['scid mice', 'mice', 'mouse', 'rabbit', 'rat', 'dog', 'goat', 'sheep', 'pig', 'cat', 'primate']\n",
    "all_cell_lines = joblib.load('all_cell_lines.sav')\n",
    "canc_list = get_cancer_list(\"cancer list.txt\") ## get names of all types of cancers (total 176 types), \n",
    "quantity_list_uni=['vp', 'vp/mm', 'moi', 'micromolar', 'multiplicity','pfu', 'tcid']\n",
    "\n",
    "\"\"\"\n",
    "Calling the above functions on each patent id to fill the data dictionary with feature values\n",
    "\"\"\"\n",
    "for patent_id in data:\n",
    "    for key in data[patent_id]: # key is fig_id\n",
    "#         print(patent_id, key)\n",
    "        paragraph = data[patent_id][key]['para']\n",
    "        tumor_cell_survival_list = data[patent_id][key]['tumor/cell_survival']\n",
    "        data[patent_id][key]['experiment_type'] = get_experiment_type(paragraph,tumor_cell_survival_list)\n",
    "        \n",
    "        exp_type = data[patent_id][key]['experiment_type']\n",
    "        data[patent_id][key]['species'] = get_species(paragraph,exp_type,invivo_animals)\n",
    "        data[patent_id][key]['cancer_type'] = get_cancer_type(paragraph,canc_list)\n",
    "        data[patent_id][key]['cell_lines'] = get_cell_lines(paragraph,all_cell_lines)\n",
    "        data[patent_id][key]['quantity'] = get_quantity(paragraph,quantity_list_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[patent_id][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features_dictionary.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dumping the dictionary data as a saved model\n",
    "joblib.dump(data,'features_dictionary.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converting the dictionary data into the form of a list, so that it could be put in a excel sheet as output\n",
    "\n",
    "Headings (Column titles) for the output sheet:  patent_id, cancer type, chemo agent/ oncolytic virus, quantity, cell survival %, tumor volume, days, species, experiment type, cell lines\n",
    "\"\"\"\n",
    "result=[]\n",
    "for patent_id in data:\n",
    "    for fig in data[patent_id]:\n",
    "        rows = len(data[patent_id][fig]['tumor/cell_survival'])\n",
    "        for i in range(rows):\n",
    "            row = []\n",
    "            row.append(patent_id)\n",
    "            row.append(data[patent_id][fig]['cancer_type'])\n",
    "            row.append(data[patent_id][fig]['chemo/virus'][i])\n",
    "            row.append(data[patent_id][fig]['quantity'])\n",
    "            if data[patent_id][fig]['tumor/cell_survival'][i] == 0: #tumor\n",
    "                row.append('')\n",
    "                row.append(data[patent_id][fig]['tumor_size_value/cell_survival_value'][i])                \n",
    "            else: # cell survival\n",
    "                row.append(data[patent_id][fig]['tumor_size_value/cell_survival_value'][i])\n",
    "                row.append('')\n",
    "            row.append(data[patent_id][fig]['days'][i])\n",
    "            row.append(data[patent_id][fig]['species'])\n",
    "            row.append(data[patent_id][fig]['experiment_type'])\n",
    "            row.append(data[patent_id][fig]['cell_lines'])\n",
    "            result.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exporting the results in an excel sheet 'final_output.csv'\n",
    "\"\"\"\n",
    "df = pd.DataFrame(result,columns=['patent_id', 'cancer type', 'chemo agent/ oncolytic virus', 'quantity', 'cell survival %', 'tumor volume (in mm3)', 'days', 'species', 'experiment type', 'cell lines']) \n",
    "df.to_excel('final_output.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
